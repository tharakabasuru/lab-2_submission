{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylTH8hRrqQH0",
        "outputId": "6ed08dfd-a044-49a6-82c6-4a9a1331929f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 = [[0.2204 0.2673]\n",
            " [0.302  0.3544]] \n",
            "\n",
            " W2 = [[-2.6532 -2.6032]\n",
            " [ 1.6826  1.7326]] \n",
            "\n",
            " Output = [[0.0732 0.9333]] \n",
            " Desired output = [[0.01 0.99]] \n",
            " Error = 0.0036066162700403763\n"
          ]
        }
      ],
      "source": [
        "#Code cell <undefined>\n",
        "# %% [code]\n",
        "from IPython.display import Image\n",
        "Image(\"image.png\")\n",
        "#Execution output\n",
        "#34KB\n",
        "\n",
        "#Code cell <undefined>\n",
        "# %% [code]\n",
        "##############################################################################\n",
        "# Implementation of a A Step by Step Backpropagation Example by Matt Mazur\n",
        "# https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
        "##############################################################################\n",
        "\n",
        "# Original code: https://github.com/vendidad/DS-repo/blob/master/Backpropagation%20-%20Consolidated%20Script.ipynb\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=4)\n",
        "\n",
        "def initialize():\n",
        "    X = np.array([[0.05, 0.10]])      # Inputs\n",
        "    W1 = np.array([[0.15,0.20], [0.25,0.30]])      # Weights to calculate outputs for hidden layer 1\n",
        "    b1 = 0.35      # Bias for hidden layer 1\n",
        "    W2 = np.array([[0.40,0.45], [0.50,0.55]])     # Weights to calculate outputs for output layer\n",
        "    b2 = 0.60      # Bias for output layer\n",
        "    Y = np.array([[0.01, 0.99]])      # Desired output\n",
        "    learning_rate = 0.5\n",
        "    no_of_iter = int(400)\n",
        "    return (X, W1, b1, W2, b2, Y, learning_rate, no_of_iter)\n",
        "\n",
        "def forward_pass (X, W1, b1, W2, b2, Y):\n",
        "    ### Forward pass: Calculate hidden layer 1 (there is only 1 hidden layer in this example)\n",
        "    Z1 = np.dot(X,W1.T) + b1      # WtX + b\n",
        "    A1 = 1/(1 + np.exp(-Z1))       # Sigmoid(z) = 1 / (1 + e^(-z))\n",
        "    ### Forward pass: Calculate output layer\n",
        "    Z2 = np.dot(A1,W2.T) + b2      # WtX + b\n",
        "    A2 = 1/(1 + np.exp(-Z2))       # Sigmoid(z) = 1 / (1 + e^(-z))\n",
        "    ### Calculate error/cost function\n",
        "    E = np.sum(1/2*np.square(Y - A2))      # squared error function\n",
        "    return (A1, A2, E)\n",
        "\n",
        "def back_propagation(X, W1, W2, Y, A1, A2, learning_rate):\n",
        "    ### Back propogation\n",
        "    ### Adjust W2\n",
        "    dEdA2 = A2 - Y\n",
        "    dA2dZ2 = np.multiply (A2,1-A2)\n",
        "    dZ2dW2 = A1\n",
        "    dEdW2 = dEdA2 * dA2dZ2 * dZ2dW2\n",
        "    W2_adj = W2 - learning_rate * dEdW2.T\n",
        "    W2 = W2_adj\n",
        "    ### Adjust W1\n",
        "    dZ2dA1 = W2.T\n",
        "    dA1dZ1 = np.multiply(A1,1-A1)\n",
        "    dZ1dW1 = X\n",
        "    dEdW1 = dEdA2 * dA2dZ2 * dZ2dA1 * dA1dZ1 * dZ1dW1\n",
        "    W1_adj = W1 - learning_rate * dEdW1.T\n",
        "    W1 = W1_adj\n",
        "    return (W1, W2)\n",
        "\n",
        "def main():\n",
        "    (X, W1, b1, W2, b2, Y, learning_rate, no_of_iter) = initialize()\n",
        "    for iter in range (0,no_of_iter):\n",
        "        (A1, A2, E) = forward_pass(X, W1, b1, W2, b2, Y)\n",
        "        (W1, W2) = back_propagation(X, W1, W2, Y, A1, A2, learning_rate)\n",
        "    print ('W1 = {} \\n\\n W2 = {} \\n\\n Output = {} \\n Desired output = {} \\n Error = {}'.format(W1, W2, A2, Y, E))\n",
        "\n",
        "main()\n",
        "#Execution output\n",
        "#0KB\n",
        "#\tStream\n",
        "#\t\tW1 = [[ 0.1826  0.2304]\n",
        "#\t\t [ 0.2705  0.3217]]\n",
        "\n",
        "#\t\t W2 = [[-1.819  -1.769 ]\n",
        "#\t\t [ 1.1239  1.1739]]\n",
        "\n",
        "#\t\t Output = [[ 0.178   0.8771]]\n",
        "#\t\t Desired output = [[ 0.01  0.99]]\n",
        "#\t\t Error = 0.020492276763740064\n",
        "\n"
      ]
    }
  ]
}